{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayteaftw/NLP-quora-duplicate-detection/blob/main/quora_duplicate_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9B9RfXUgKJV"
      },
      "source": [
        "We are only using test.csv(400k training examples). We are gonna do train, cv, test split on it. Lets just do a simple logistic regression on this and set a baseline performance for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5uMyRMhNdLT",
        "outputId": "f6ee4134-cd5f-4b86-f98d-e24ca2ba9175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path is \n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive/')\n",
        "    path = '/content/gdrive/MyDrive/NLP_Final_Project/' \n",
        "except:\n",
        "    path = ''\n",
        "\n",
        "print(f\"Path is {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "omhBufyrSTDt"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUYmQGxEi8XM",
        "outputId": "5f4c9635-3e46-40b6-d058-99b456b0ae49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW2U2MJAY_MW"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnqG78znbHWJ",
        "outputId": "953e4dbe-307c-43e2-a8e0-90f451379770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(404290,)\n",
            "(404290, 5)\n",
            "149263\n",
            "Ratio:\n",
            " 1: 36.9197853026293% \n",
            " 0: 63.08021469737071%\n"
          ]
        }
      ],
      "source": [
        "path = ''\n",
        "df = pd.read_csv(path+'train.csv')\n",
        "y = np.array(df['is_duplicate'])\n",
        "X = df[['id', 'qid1', 'qid2', 'question1', 'question2']]\n",
        "X = np.array(X)\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "ones = np.count_nonzero(y == 1)\n",
        "print(ones)\n",
        "print(f\"Ratio:\\n 1: {(ones/len(y)) * 100}% \\n 0: {(1-ones/len(y)) * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYYTtlNqbJka",
        "outputId": "08d2a683-40ca-44c2-9d80-91c0bb25605c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['What is the step by step guide to invest in share market in india?',\n",
              "       'What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n",
              "       'How can I increase the speed of my internet connection while using a VPN?',\n",
              "       ..., 'What is one coin?',\n",
              "       'What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?',\n",
              "       'What is like to have sex with cousin?'], dtype=object)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train = df[['question1','question2' ]]\n",
        "X_train = np.array(X_train)\n",
        "X_train[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "-pVsecKfZB-1"
      },
      "outputs": [],
      "source": [
        "def train(train_dataset, test_dataset,feature_size, model, params={}, print_step=10):\n",
        "    train_len = len(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset,batch_size=64)\n",
        "    \n",
        "    if test_dataset != None:\n",
        "        test_len = len(test_dataset)\n",
        "        test_dataloader = DataLoader(test_dataset,batch_size=64)\n",
        "\n",
        "    \n",
        "\n",
        "    loss_fn = params['loss']\n",
        "    epochs = params['epochs']\n",
        "    optimizer = params['optimizer']\n",
        "\n",
        "    \n",
        "\n",
        "    model.to(device)\n",
        "    epoch_history = []\n",
        "    fold_train_loss_history, fold_val_loss_history = [], []\n",
        "    fold_val_acc_history = []\n",
        "    \n",
        "    for e in range(1,epochs+1):\n",
        "        \n",
        "        total_train_loss = 0\n",
        "        total_train_correct = 0\n",
        "        for batch_x, batch_y in train_dataloader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "    \n",
        "            # Compute prediction error\n",
        "            pred = torch.squeeze(model(batch_x))\n",
        "            loss = loss_fn(pred, batch_y)\n",
        "            \n",
        "            \n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #Save Training Loss and total correct\n",
        "            total_train_loss += loss.item()\n",
        "            out = (pred>0.5).float()\n",
        "            total_train_correct += sum(out==batch_y).float().sum()\n",
        "     \n",
        "        if e == 1 or e % print_step == 0:\n",
        "            with torch.no_grad():\n",
        "                epoch_history.append(e)\n",
        "\n",
        "                train_acc = total_train_correct / train_len\n",
        "                avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "                fold_train_loss_history.append(avg_train_loss)\n",
        "                if test_dataset != None:\n",
        "                    total_val_correct = 0\n",
        "                    total_val_loss = 0\n",
        "                    for val_x, val_y in test_dataloader:\n",
        "                        val_x, val_y = val_x.to(device), val_y.to(device)\n",
        "                        val_pred = torch.squeeze(model(val_x))\n",
        "                        val_out = (val_pred>0.5).float()\n",
        "                        val_loss = loss_fn(val_pred, val_y)\n",
        "                        total_val_loss += val_loss.cpu()\n",
        "                        total_val_correct += sum(val_out==val_y).float().sum()\n",
        "\n",
        "                    val_acc = total_val_correct / test_len\n",
        "                    avg_val_loss = total_val_loss / len(test_dataloader)\n",
        "                    fold_val_acc_history.append(val_acc.cpu())\n",
        "                    fold_val_loss_history.append(avg_val_loss)\n",
        "               \n",
        "\n",
        "                    print(f\"epoch: {e}, Train loss: {avg_train_loss:>4f},  Train Acc: {train_acc:>4f}, Val Loss: {avg_val_loss::>4f}, Val Acc: {val_acc:>4f}\")\n",
        "                else:\n",
        "                    print(f\"epoch: {e}, Train loss: {avg_train_loss:>4f},  Train Acc: {train_acc:>4f}\")   \n",
        "    return fold_train_loss_history, fold_val_loss_history, fold_val_acc_history, epoch_history,model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdugqPIi9iGm"
      },
      "source": [
        "# Glove50 Logsitic Regregression Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "XiOtdSh1UrYi"
      },
      "outputs": [],
      "source": [
        "class Glove50Dataset():\n",
        "    def __init__(self,X,y )-> None:\n",
        "        \n",
        "\n",
        "        word_embeddings = pd.read_csv(path+'glove.6B.50d.txt.zip',\n",
        "                               header=None, sep=' ', index_col=0,\n",
        "                               nrows=100000, compression='zip', encoding='utf-8', quoting=3)\n",
        "        # Build a dict that will map from string word to 50-dim vector\n",
        "        word_list = word_embeddings.index.values.tolist()\n",
        "        word2vec = OrderedDict(zip(word_list, word_embeddings.values))\n",
        "        \n",
        "\n",
        "        q1 = []\n",
        "        q2 = []\n",
        "        a = []\n",
        "        for (x1, x2),y1 in zip(X,y):\n",
        "          if isinstance(x1, str) and isinstance(x2, str):\n",
        "            q1.append(x1)\n",
        "            q2.append(x2)\n",
        "            a.append(y1)\n",
        "          else:\n",
        "            print(x1,x2)\n",
        "\n",
        "        pair_len = len(q1)\n",
        "\n",
        "        x_embed_q1 = np.zeros((pair_len, 50))\n",
        "        x_embed_q2 = np.zeros((pair_len, 50))        \n",
        "        for idx, (x1, x2) in enumerate(zip(q1,q2)):\n",
        "\n",
        "          question1 = x1.strip().split(\" \")\n",
        "          question2 = x2.strip().split(\" \")\n",
        "\n",
        "          for word in question1:\n",
        "            if word in word2vec:\n",
        "              x_embed_q1[idx] += word2vec[word]\n",
        "            x_embed_q1[idx] /= len(question1)\n",
        "\n",
        "          for word in question2:\n",
        "            if word in word2vec:\n",
        "              x_embed_q2[idx] += word2vec[word]\n",
        "            x_embed_q2[idx] /= len(question2)\n",
        "        \n",
        "\n",
        "        x_embed = np.concatenate((x_embed_q1,x_embed_q2),axis=1)\n",
        "        print(x_embed.shape)\n",
        "\n",
        "        self.feature_size = x_embed.shape[1]\n",
        "        self.x_train = torch.Tensor(x_embed)[:100000]\n",
        "        self.y_train = torch.Tensor(a)[:100000]\n",
        "        print(self.feature_size, self.x_train.shape, self.y_train.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_train.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_train[idx], self.y_train[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "5VqDBOcWSNev"
      },
      "outputs": [],
      "source": [
        "class LogisticModel(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(LogisticModel,self).__init__()\n",
        "        self.w = nn.Linear(input_size,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.sigmoid(self.w(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPiSkOTNY0rh",
        "outputId": "c9d8e0fa-59bd-4747-a5bb-2f44d7a7baf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How can I develop android app? nan\n",
            "How can I create an Android app? nan\n",
            "nan My Chinese name is Haichao Yu. What English name is most suitable for me considering the pronounciation of my Chinese name?\n",
            "(404287, 100)\n",
            "100 torch.Size([100000, 100]) torch.Size([100000])\n"
          ]
        }
      ],
      "source": [
        "dataset = Glove50Dataset(X_train,y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "-ruINwqrgU00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fold 0\n",
            "epoch: 1, Train loss: 0.654246,  Train Acc: 0.627133, Val Loss: 0.647278, Val Acc: 0.631700\n",
            "epoch: 10, Train loss: 0.649053,  Train Acc: 0.632844, Val Loss: 0.645101, Val Acc: 0.636400\n",
            "epoch: 20, Train loss: 0.649007,  Train Acc: 0.633256, Val Loss: 0.645057, Val Acc: 0.636400\n",
            "epoch: 30, Train loss: 0.648986,  Train Acc: 0.633444, Val Loss: 0.645047, Val Acc: 0.636400\n",
            "epoch: 40, Train loss: 0.648968,  Train Acc: 0.633444, Val Loss: 0.645041, Val Acc: 0.636300\n",
            "epoch: 50, Train loss: 0.648952,  Train Acc: 0.633311, Val Loss: 0.645036, Val Acc: 0.636200\n",
            "epoch: 60, Train loss: 0.648938,  Train Acc: 0.633311, Val Loss: 0.645032, Val Acc: 0.636000\n",
            "F1 score: 0.130019120458891\n",
            "AUC score: 0.6087516083096625\n",
            "Starting fold 1\n",
            "epoch: 1, Train loss: 0.648253,  Train Acc: 0.633856, Val Loss: 0.650314, Val Acc: 0.631700\n",
            "epoch: 10, Train loss: 0.648180,  Train Acc: 0.634000, Val Loss: 0.650716, Val Acc: 0.632200\n",
            "epoch: 20, Train loss: 0.648170,  Train Acc: 0.633944, Val Loss: 0.650770, Val Acc: 0.632200\n",
            "epoch: 30, Train loss: 0.648162,  Train Acc: 0.633911, Val Loss: 0.650775, Val Acc: 0.632100\n",
            "epoch: 40, Train loss: 0.648154,  Train Acc: 0.633844, Val Loss: 0.650771, Val Acc: 0.632100\n",
            "epoch: 50, Train loss: 0.648148,  Train Acc: 0.633889, Val Loss: 0.650766, Val Acc: 0.632000\n",
            "epoch: 60, Train loss: 0.648142,  Train Acc: 0.633900, Val Loss: 0.650761, Val Acc: 0.632000\n",
            "F1 score: 0.13696060037523453\n",
            "AUC score: 0.5934758050378199\n",
            "Starting fold 2\n",
            "epoch: 1, Train loss: 0.648672,  Train Acc: 0.632400, Val Loss: 0.646832, Val Acc: 0.638500\n",
            "epoch: 10, Train loss: 0.648602,  Train Acc: 0.632678, Val Loss: 0.647562, Val Acc: 0.638400\n",
            "epoch: 20, Train loss: 0.648591,  Train Acc: 0.632644, Val Loss: 0.647669, Val Acc: 0.638400\n",
            "epoch: 30, Train loss: 0.648583,  Train Acc: 0.632544, Val Loss: 0.647700, Val Acc: 0.638000\n",
            "epoch: 40, Train loss: 0.648577,  Train Acc: 0.632544, Val Loss: 0.647717, Val Acc: 0.637900\n",
            "epoch: 50, Train loss: 0.648572,  Train Acc: 0.632522, Val Loss: 0.647730, Val Acc: 0.637900\n",
            "epoch: 60, Train loss: 0.648568,  Train Acc: 0.632600, Val Loss: 0.647743, Val Acc: 0.638200\n",
            "F1 score: 0.15348619560131027\n",
            "AUC score: 0.5973879260676965\n",
            "Starting fold 3\n",
            "epoch: 1, Train loss: 0.648969,  Train Acc: 0.632478, Val Loss: 0.643744, Val Acc: 0.639900\n",
            "epoch: 10, Train loss: 0.648883,  Train Acc: 0.632489, Val Loss: 0.644527, Val Acc: 0.639900\n",
            "epoch: 20, Train loss: 0.648879,  Train Acc: 0.632511, Val Loss: 0.644606, Val Acc: 0.639700\n",
            "epoch: 30, Train loss: 0.648876,  Train Acc: 0.632433, Val Loss: 0.644623, Val Acc: 0.639700\n",
            "epoch: 40, Train loss: 0.648873,  Train Acc: 0.632422, Val Loss: 0.644631, Val Acc: 0.639700\n",
            "epoch: 50, Train loss: 0.648871,  Train Acc: 0.632444, Val Loss: 0.644637, Val Acc: 0.639600\n",
            "epoch: 60, Train loss: 0.648868,  Train Acc: 0.632456, Val Loss: 0.644642, Val Acc: 0.639600\n",
            "F1 score: 0.1414959504525965\n",
            "AUC score: 0.6067290478237644\n",
            "Starting fold 4\n",
            "epoch: 1, Train loss: 0.648082,  Train Acc: 0.633967, Val Loss: 0.651338, Val Acc: 0.629700\n",
            "epoch: 10, Train loss: 0.647968,  Train Acc: 0.634267, Val Loss: 0.652189, Val Acc: 0.629100\n",
            "epoch: 20, Train loss: 0.647964,  Train Acc: 0.634322, Val Loss: 0.652290, Val Acc: 0.628900\n",
            "epoch: 30, Train loss: 0.647962,  Train Acc: 0.634278, Val Loss: 0.652305, Val Acc: 0.628800\n",
            "epoch: 40, Train loss: 0.647961,  Train Acc: 0.634256, Val Loss: 0.652309, Val Acc: 0.628800\n",
            "epoch: 50, Train loss: 0.647960,  Train Acc: 0.634289, Val Loss: 0.652311, Val Acc: 0.628800\n",
            "epoch: 60, Train loss: 0.647958,  Train Acc: 0.634311, Val Loss: 0.652313, Val Acc: 0.628800\n",
            "F1 score: 0.14074074074074075\n",
            "AUC score: 0.5919814627588504\n",
            "Starting fold 5\n",
            "epoch: 1, Train loss: 0.648321,  Train Acc: 0.633889, Val Loss: 0.649463, Val Acc: 0.631800\n",
            "epoch: 10, Train loss: 0.648211,  Train Acc: 0.633811, Val Loss: 0.650220, Val Acc: 0.630600\n",
            "epoch: 20, Train loss: 0.648209,  Train Acc: 0.633911, Val Loss: 0.650294, Val Acc: 0.630300\n",
            "epoch: 30, Train loss: 0.648208,  Train Acc: 0.633900, Val Loss: 0.650304, Val Acc: 0.630100\n",
            "epoch: 40, Train loss: 0.648206,  Train Acc: 0.633856, Val Loss: 0.650306, Val Acc: 0.630000\n",
            "epoch: 50, Train loss: 0.648205,  Train Acc: 0.633844, Val Loss: 0.650307, Val Acc: 0.630000\n",
            "epoch: 60, Train loss: 0.648204,  Train Acc: 0.633878, Val Loss: 0.650307, Val Acc: 0.630000\n",
            "F1 score: 0.15215398716773604\n",
            "AUC score: 0.6049933011789925\n",
            "Starting fold 6\n",
            "epoch: 1, Train loss: 0.648480,  Train Acc: 0.633722, Val Loss: 0.647694, Val Acc: 0.635800\n",
            "epoch: 10, Train loss: 0.648409,  Train Acc: 0.633433, Val Loss: 0.648179, Val Acc: 0.635200\n",
            "epoch: 20, Train loss: 0.648407,  Train Acc: 0.633433, Val Loss: 0.648225, Val Acc: 0.635400\n",
            "epoch: 30, Train loss: 0.648405,  Train Acc: 0.633411, Val Loss: 0.648234, Val Acc: 0.635300\n",
            "epoch: 40, Train loss: 0.648404,  Train Acc: 0.633500, Val Loss: 0.648238, Val Acc: 0.635200\n",
            "epoch: 50, Train loss: 0.648403,  Train Acc: 0.633544, Val Loss: 0.648241, Val Acc: 0.635200\n",
            "epoch: 60, Train loss: 0.648402,  Train Acc: 0.633589, Val Loss: 0.648244, Val Acc: 0.635200\n",
            "F1 score: 0.14083843617522374\n",
            "AUC score: 0.5945841774226889\n",
            "Starting fold 7\n",
            "epoch: 1, Train loss: 0.648732,  Train Acc: 0.632878, Val Loss: 0.645856, Val Acc: 0.635900\n",
            "epoch: 10, Train loss: 0.648644,  Train Acc: 0.632978, Val Loss: 0.646620, Val Acc: 0.635400\n",
            "epoch: 20, Train loss: 0.648641,  Train Acc: 0.632844, Val Loss: 0.646703, Val Acc: 0.635500\n",
            "epoch: 30, Train loss: 0.648640,  Train Acc: 0.632867, Val Loss: 0.646721, Val Acc: 0.635600\n",
            "epoch: 40, Train loss: 0.648639,  Train Acc: 0.632867, Val Loss: 0.646730, Val Acc: 0.635600\n",
            "epoch: 50, Train loss: 0.648638,  Train Acc: 0.632889, Val Loss: 0.646736, Val Acc: 0.635600\n",
            "epoch: 60, Train loss: 0.648637,  Train Acc: 0.632878, Val Loss: 0.646742, Val Acc: 0.635600\n",
            "F1 score: 0.15920627595754497\n",
            "AUC score: 0.6070849275948381\n",
            "Starting fold 8\n",
            "epoch: 1, Train loss: 0.648158,  Train Acc: 0.633833, Val Loss: 0.651508, Val Acc: 0.627400\n",
            "epoch: 10, Train loss: 0.648061,  Train Acc: 0.634000, Val Loss: 0.652252, Val Acc: 0.626800\n",
            "epoch: 20, Train loss: 0.648057,  Train Acc: 0.633978, Val Loss: 0.652350, Val Acc: 0.626900\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[86], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m x_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(x_test)\n\u001b[1;32m     26\u001b[0m y_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y_test)\n\u001b[1;32m     29\u001b[0m fold_train_loss_history, fold_val_loss_history, \\\n\u001b[0;32m---> 30\u001b[0m fold_val_acc_history, epoch_history, _ \u001b[39m=\u001b[39m train(\\\n\u001b[1;32m     31\u001b[0m                         train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,\n\u001b[1;32m     32\u001b[0m                         test_dataset\u001b[39m=\u001b[39;49mtest_dataset,\n\u001b[1;32m     33\u001b[0m                         feature_size\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mfeature_size,\n\u001b[1;32m     34\u001b[0m                         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     35\u001b[0m                         params \u001b[39m=\u001b[39;49m params)\n\u001b[1;32m     38\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(model(x_test))\n\u001b[1;32m     39\u001b[0m y_pred \u001b[39m=\u001b[39m (pred\u001b[39m>\u001b[39m\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat()\n",
            "Cell \u001b[0;32mIn[82], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataset, test_dataset, feature_size, model, params, print_step)\u001b[0m\n\u001b[1;32m     40\u001b[0m     total_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     41\u001b[0m     out \u001b[39m=\u001b[39m (pred\u001b[39m>\u001b[39m\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 42\u001b[0m     total_train_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(out\u001b[39m==\u001b[39;49mbatch_y)\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39msum()\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m e \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m e \u001b[39m%\u001b[39m print_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#For Testing\n",
        "model = LogisticModel(dataset.feature_size) \n",
        "\n",
        "params = {  \"loss\": nn.BCELoss(),\n",
        "            \"optimizer\":torch.optim.Adam(model.parameters(), lr=1e-2,betas=(0.9, 0.999)),\n",
        "            \"epochs\": 60 }\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "kfold.split(dataset)\n",
        "train_loss_history, val_loss_history = [], []\n",
        "val_acc_history = []\n",
        "for fold, (train_idxs, test_idxs) in enumerate(kfold.split(dataset)):\n",
        "\n",
        "    print(f'Starting fold {fold}')\n",
        "    train_dataset = torch.utils.data.Subset(dataset,train_idxs)\n",
        "    test_dataset = torch.utils.data.Subset(dataset,test_idxs)\n",
        "\n",
        "\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for idx in range(len(test_dataset)):\n",
        "        input_data, target = test_dataset[idx]\n",
        "        x_test.append(input_data)\n",
        "        y_test.append(target)\n",
        "    x_test = torch.stack(x_test)\n",
        "    y_test = torch.tensor(y_test)\n",
        "\n",
        "\n",
        "    fold_train_loss_history, fold_val_loss_history, \\\n",
        "    fold_val_acc_history, epoch_history, _ = train(\\\n",
        "                            train_dataset=train_dataset,\n",
        "                            test_dataset=test_dataset,\n",
        "                            feature_size=dataset.feature_size,\n",
        "                            model=model,\n",
        "                            params = params)\n",
        "    \n",
        "\n",
        "    pred = torch.squeeze(model(x_test))\n",
        "    y_pred = (pred>0.5).float()\n",
        "    print(f\"F1 score: {f1_score(y_test, y_pred, pos_label=1, average='binary')}\")\n",
        "    print(f\"AUC score: {roc_auc_score(y_test.detach().numpy(), model.forward(x_test).detach().numpy())}\")\n",
        "\n",
        "    \n",
        "    train_loss_history.append(fold_train_loss_history)\n",
        "    val_loss_history.append(fold_val_loss_history)\n",
        "    val_acc_history.append(fold_val_acc_history) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITY2hjQKTDM4"
      },
      "source": [
        "# Sentence Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Evn_hzLWDRoh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.24.2)\n",
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.5.5)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->sentence-transformers) (9.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, fsspec, huggingface-hub, transformers, sentence-transformers\n",
            "\u001b[33m  DEPRECATION: sentence-transformers is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py install for sentence-transformers ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed fsspec-2023.5.0 huggingface-hub-0.15.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "_EjEhMElDiDf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9rbCmavDOAp",
        "outputId": "2eac146f-cb76-440d-8d7d-a8c20371d93c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "class SentenceTransformerDataset():\n",
        "    def __init__(self,X,y, pair_len=0)-> None:\n",
        "        \n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        \n",
        "        q1 = []\n",
        "        q2 = []\n",
        "        a = []\n",
        "        for (x1, x2),y1 in zip(X,y):\n",
        "          if isinstance(x1, str) and isinstance(x2, str):\n",
        "            q1.append(x1)\n",
        "            q2.append(x2)\n",
        "            a.append(y1)\n",
        "          else:\n",
        "            print(x1,x2)\n",
        "\n",
        "        pair_len = pair_len if pair_len else len(q1)\n",
        "\n",
        "        q1, q2, a = q1[:pair_len], q2[:pair_len], a[:pair_len]\n",
        "        x_embed_q1 = np.zeros((pair_len, 384))[:pair_len]\n",
        "        x_embed_q2 = np.zeros((pair_len, 384)) [:pair_len]\n",
        "\n",
        "        pool = model.start_multi_process_pool()\n",
        "\n",
        "\n",
        "        #Start the multi-process pool on all available CUDA devices\n",
        "        pool = model.start_multi_process_pool()\n",
        "\n",
        "        step_size = 10000\n",
        "        for start in tqdm(range(0, pair_len, step_size),total=pair_len//step_size):\n",
        "          stop = start + step_size if (start + step_size) < pair_len else (pair_len - 1)\n",
        "          x_embed_q1[start:stop] = model.encode_multi_process(q1[start:stop], pool)\n",
        "          x_embed_q2[start:stop] = model.encode_multi_process(q2[start:stop], pool)\n",
        "\n",
        "        #Optional: Stop the proccesses in the pool\n",
        "        model.stop_multi_process_pool(pool)\n",
        "\n",
        "        x_embed = np.concatenate((x_embed_q1,x_embed_q2),axis=1)\n",
        "        \n",
        "        print(x_embed.shape)\n",
        "\n",
        "        self.feature_size = x_embed.shape[1]\n",
        "        self.setences = [q1,q2]\n",
        "        self.x_train = torch.Tensor(x_embed)\n",
        "        self.y_train = torch.Tensor(a)\n",
        "        print(self.feature_size, self.x_train.shape, self.y_train.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_train.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_train[idx], self.y_train[idx]\n",
        "\n",
        "\n",
        "\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Kqx5xWXThD",
        "outputId": "4c8697e0-8165-4e7b-b37c-921f92824178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Dataset from Drive\n",
            "Dataset Load!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "create_bert_encoding = False\n",
        "file_name = 'bert_encoding_max.pkl'\n",
        "\n",
        "\n",
        "run = 'y'\n",
        "if create_bert_encoding: \n",
        "  if os.path.isfile(path+file_name):\n",
        "    run = input(f\"'{file_name}' exists in the drive! Would you like to replace? (y/n)\")\n",
        "  if run == 'y':\n",
        "    bert_dataset = SentenceTransformerDataset(X_train,y,pair_len=0)\n",
        "    # Save the object to a file\n",
        "    print(f'Writing Dataset to Drive')\n",
        "    with open(path+file_name, 'wb') as f:\n",
        "        pickle.dump(bert_dataset, f)\n",
        "    print(f'Dataset Written!')\n",
        "\n",
        "if not create_bert_encoding or run != 'y':\n",
        "  # Load the object from the file\n",
        "  if not os.path.isfile(path+file_name):\n",
        "    raise ValueError(\"No Dataset in drive\")\n",
        "  else:\n",
        "    print(f'Loading Dataset from Drive')\n",
        "    with open(path+file_name, 'rb') as f:\n",
        "        bert_dataset = pickle.load(f)\n",
        "    print(f'Dataset Load!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6unhCuBzzih3",
        "outputId": "84e0b1ff-ebce-4587-abf2-63a2a0226e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train|Val Percent: 0.8|0.19999999999999996\n",
            "Train|Val Size: 323429|80858\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "train_percent = .8\n",
        "val_percent = 1 - train_percent\n",
        "sample_size = len(bert_dataset)\n",
        "train_size = int(sample_size * train_percent)\n",
        "val_size = sample_size - train_size\n",
        "\n",
        "\n",
        "print(f'Train|Val Percent: {train_percent}|{val_percent}')\n",
        "print(f'Train|Val Size: {train_size}|{val_size}')\n",
        "# Split the data into train and test sets\n",
        "train_dataset, test_dataset = random_split(bert_dataset, [train_size, val_size])\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "for idx in range(len(test_dataset)):\n",
        "    input_data, target = test_dataset[idx]\n",
        "    x_test.append(input_data)\n",
        "    y_test.append(target)\n",
        "x_test = torch.stack(x_test)\n",
        "y_test = torch.tensor(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of 0s: 204207\n",
            "Number of 1s: 119222\n",
            "Training data Ratio:\n",
            " 1: 36.8618769498097% \n",
            " 0: 63.13812305019031%\n"
          ]
        }
      ],
      "source": [
        "# Check data balance\n",
        "indices = train_dataset.indices\n",
        "targets = train_dataset.dataset.y_train\n",
        "\n",
        "num_zeros = 0\n",
        "num_ones = 0\n",
        "\n",
        "for index in indices:\n",
        "    label = targets[index]\n",
        "    if label == 0:\n",
        "        num_zeros += 1\n",
        "    elif label == 1:\n",
        "        num_ones += 1\n",
        "\n",
        "print(\"Number of 0s:\", num_zeros)\n",
        "print(\"Number of 1s:\", num_ones)\n",
        "ratio = num_ones / (num_ones + num_zeros)\n",
        "print(f\"Training data Ratio:\\n 1: {ratio * 100}% \\n 0: {(1-ratio) * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a3dda4d00b524a16b8ddfabea43f3c07",
            "a119a8e7f5574e1e85965abd5749b91b",
            "2d79d95f737b4c34905e0a7f40f91220",
            "69d4cb37419f4a448140d7d1498a2fd5",
            "e194987ed32a4bd6b0b6d18ef0cf1da3",
            "64b3ef56796747fb8d9ef8cf8b4e0171",
            "5021a6b89ed945bf94483e1f9473db3f",
            "f1c5786153924fce9a3a52aac1c1570d",
            "4b2c4fc257474af282c8ba74bc140757",
            "3d79872fbecd4fc6a59cf9b3a01d3581",
            "a99922cd7d32404fb1c746b6fe575056"
          ]
        },
        "id": "5UW1ea5nkIe3",
        "outputId": "b6d29dd5-62a5-4323-dc0f-5a1aca302035"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 80858/80858 [00:02<00:00, 36971.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Sim Accuracy: 0.39657176779044745\n",
            "F1 score: 0.551848926281757\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import util\n",
        "import math\n",
        "\n",
        "feature_size = bert_dataset.feature_size//2\n",
        "total = 0\n",
        "cos_scores = []\n",
        "for x, y in tqdm(test_dataset, total=len(test_dataset)):\n",
        "  \n",
        "  q1 = x[:feature_size]\n",
        "  q2 = x[feature_size:]\n",
        "  cosine = util.cos_sim(q1, q2).item()\n",
        "  cos_scores.append(1 if cosine >=0 else 0)\n",
        "  if cosine >= 0 and y == 1 or cosine < 0 and y == 0:\n",
        "    total += 1\n",
        "\n",
        "percent = total / len(test_dataset)\n",
        "print(f\"Cosine Sim Accuracy: {percent}\")\n",
        "print(f\"F1 score: {f1_score(y_test, cos_scores, average='binary')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJBS83PJeFmG",
        "outputId": "5c673554-6cdb-4b40-fb43-7973673eb955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, Train loss: 0.572727,  Train Acc: 0.708653, Val Loss: 0.567628, Val Acc: 0.713832\n",
            "epoch: 2, Train loss: 0.566584,  Train Acc: 0.713000, Val Loss: 0.566740, Val Acc: 0.714215\n",
            "epoch: 4, Train loss: 0.565799,  Train Acc: 0.713464, Val Loss: 0.566353, Val Acc: 0.714561\n",
            "epoch: 6, Train loss: 0.565677,  Train Acc: 0.713585, Val Loss: 0.566254, Val Acc: 0.714957\n",
            "epoch: 8, Train loss: 0.565646,  Train Acc: 0.713588, Val Loss: 0.566215, Val Acc: 0.715217\n",
            "epoch: 10, Train loss: 0.565636,  Train Acc: 0.713498, Val Loss: 0.566196, Val Acc: 0.715093\n",
            "F1 score: 0.5459705551942292\n",
            "AUC score: 0.7478245615112822\n"
          ]
        }
      ],
      "source": [
        "#For Training Simple Logisitc Regression with Bert\n",
        "\n",
        "model = LogisticModel(bert_dataset.feature_size) \n",
        "\n",
        "params = {  \"loss\": nn.BCELoss(),\n",
        "            \"optimizer\":torch.optim.Adam(model.parameters(), lr=1e-2,betas=(0.9, 0.999)),\n",
        "            \"epochs\": 10 }\n",
        "\n",
        "\n",
        "fold_train_loss_history, fold_val_loss_history, \\\n",
        "    fold_val_acc_history, epoch_history, _ = train(\\\n",
        "                            train_dataset=train_dataset,\n",
        "                            test_dataset=test_dataset,\n",
        "                            feature_size=bert_dataset.feature_size,\n",
        "                            model=model,\n",
        "                            params = params,\n",
        "                            print_step=2)\n",
        "\n",
        "pred = torch.squeeze(model(x_test))\n",
        "y_pred = (pred>0.5).float()\n",
        "print(f\"F1 score: {f1_score(y_test, y_pred, pos_label=1, average='binary')}\")\n",
        "print(f\"AUC score: {roc_auc_score(y_test.detach().numpy(), model.forward(x_test).detach().numpy())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48\n"
          ]
        }
      ],
      "source": [
        "print(bert_dataset.feature_size//2//2//2//2)\n",
        "class LargerBertNetwork(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(LargerBertNetwork,self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size,384)\n",
        "        self.layer_2 = nn.Linear(384,192)\n",
        "        self.layer_3 = nn.Linear(192,96)\n",
        "        self.layer_4 = nn.Linear(96,48)\n",
        "        self.layer_f = nn.Linear(48,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        x = F.relu(self.layer_3(x))\n",
        "        x = F.relu(self.layer_4(x))\n",
        "        return F.sigmoid(self.layer_f(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, Train loss: 0.621545,  Train Acc: 0.649330, Val Loss: 0.579493, Val Acc: 0.713485\n",
            "epoch: 2, Train loss: 0.553394,  Train Acc: 0.727424, Val Loss: 0.532930, Val Acc: 0.740335\n",
            "epoch: 3, Train loss: 0.498440,  Train Acc: 0.761104, Val Loss: 0.465398, Val Acc: 0.777301\n",
            "epoch: 4, Train loss: 0.423719,  Train Acc: 0.799471, Val Loss: 0.397122, Val Acc: 0.811682\n",
            "epoch: 5, Train loss: 0.370333,  Train Acc: 0.828006, Val Loss: 0.365030, Val Acc: 0.827648\n",
            "epoch: 6, Train loss: 0.343200,  Train Acc: 0.842939, Val Loss: 0.350163, Val Acc: 0.835848\n",
            "epoch: 7, Train loss: 0.326063,  Train Acc: 0.852054, Val Loss: 0.341370, Val Acc: 0.841042\n",
            "epoch: 8, Train loss: 0.312984,  Train Acc: 0.859366, Val Loss: 0.335446, Val Acc: 0.844715\n",
            "epoch: 9, Train loss: 0.302110,  Train Acc: 0.865430, Val Loss: 0.331268, Val Acc: 0.847275\n",
            "epoch: 10, Train loss: 0.292616,  Train Acc: 0.870488, Val Loss: 0.328285, Val Acc: 0.849440\n",
            "epoch: 11, Train loss: 0.284088,  Train Acc: 0.875323, Val Loss: 0.326222, Val Acc: 0.851159\n",
            "epoch: 12, Train loss: 0.276280,  Train Acc: 0.879442, Val Loss: 0.324830, Val Acc: 0.852754\n",
            "epoch: 13, Train loss: 0.269027,  Train Acc: 0.883455, Val Loss: 0.324056, Val Acc: 0.853558\n",
            "epoch: 14, Train loss: 0.262200,  Train Acc: 0.886992, Val Loss: 0.323808, Val Acc: 0.854659\n",
            "epoch: 15, Train loss: 0.255739,  Train Acc: 0.890427, Val Loss: 0.323976, Val Acc: 0.855834\n",
            "epoch: 16, Train loss: 0.249589,  Train Acc: 0.893615, Val Loss: 0.324550, Val Acc: 0.856242\n",
            "epoch: 17, Train loss: 0.243689,  Train Acc: 0.896611, Val Loss: 0.325492, Val Acc: 0.856885\n",
            "epoch: 18, Train loss: 0.237987,  Train Acc: 0.899728, Val Loss: 0.326828, Val Acc: 0.857318\n",
            "epoch: 19, Train loss: 0.232487,  Train Acc: 0.902452, Val Loss: 0.328489, Val Acc: 0.857479\n",
            "epoch: 20, Train loss: 0.227136,  Train Acc: 0.905160, Val Loss: 0.330451, Val Acc: 0.857565\n",
            "F1 score: 0.8094883628604039\n",
            "AUC score: 0.9335005512603727\n"
          ]
        }
      ],
      "source": [
        "#For Training Largers Model with Bert\n",
        "model = LargerBertNetwork(bert_dataset.feature_size) \n",
        "\n",
        "params = {  \"loss\": nn.BCELoss(),\n",
        "            \"optimizer\":torch.optim.Adam(model.parameters(), lr=1e-5,betas=(0.9, 0.999)),\n",
        "            \"epochs\": 20 }\n",
        "\n",
        "\n",
        "fold_train_loss_history, fold_val_loss_history, \\\n",
        "    fold_val_acc_history, epoch_history, _ = train(\\\n",
        "                            train_dataset=train_dataset,\n",
        "                            test_dataset=test_dataset,\n",
        "                            feature_size=bert_dataset.feature_size,\n",
        "                            model=model,\n",
        "                            params = params,\n",
        "                            print_step=1)\n",
        "\n",
        "pred = torch.squeeze(model(x_test))\n",
        "y_pred = (pred>0.5).float()\n",
        "print(f\"F1 score: {f1_score(y_test, y_pred, pos_label=1, average='binary')}\")\n",
        "print(f\"AUC score: {roc_auc_score(y_test.detach().numpy(), model.forward(x_test).detach().numpy())}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PdugqPIi9iGm"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d79d95f737b4c34905e0a7f40f91220": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c5786153924fce9a3a52aac1c1570d",
            "max": 80858,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b2c4fc257474af282c8ba74bc140757",
            "value": 80858
          }
        },
        "3d79872fbecd4fc6a59cf9b3a01d3581": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2c4fc257474af282c8ba74bc140757": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5021a6b89ed945bf94483e1f9473db3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64b3ef56796747fb8d9ef8cf8b4e0171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d4cb37419f4a448140d7d1498a2fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d79872fbecd4fc6a59cf9b3a01d3581",
            "placeholder": "​",
            "style": "IPY_MODEL_a99922cd7d32404fb1c746b6fe575056",
            "value": " 80858/80858 [00:10&lt;00:00, 6166.60it/s]"
          }
        },
        "a119a8e7f5574e1e85965abd5749b91b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64b3ef56796747fb8d9ef8cf8b4e0171",
            "placeholder": "​",
            "style": "IPY_MODEL_5021a6b89ed945bf94483e1f9473db3f",
            "value": "100%"
          }
        },
        "a3dda4d00b524a16b8ddfabea43f3c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a119a8e7f5574e1e85965abd5749b91b",
              "IPY_MODEL_2d79d95f737b4c34905e0a7f40f91220",
              "IPY_MODEL_69d4cb37419f4a448140d7d1498a2fd5"
            ],
            "layout": "IPY_MODEL_e194987ed32a4bd6b0b6d18ef0cf1da3"
          }
        },
        "a99922cd7d32404fb1c746b6fe575056": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e194987ed32a4bd6b0b6d18ef0cf1da3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c5786153924fce9a3a52aac1c1570d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
