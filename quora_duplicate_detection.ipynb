{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayteaftw/NLP-quora-duplicate-detection/blob/main/quora_duplicate_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9B9RfXUgKJV"
      },
      "source": [
        "We are only using test.csv(400k training examples). We are gonna do train, cv, test split on it. Lets just do a simple logistic regression on this and set a baseline performance for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5uMyRMhNdLT",
        "outputId": "f6ee4134-cd5f-4b86-f98d-e24ca2ba9175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path is \n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive/')\n",
        "    path = '/content/gdrive/MyDrive/NLP_Final_Project/' \n",
        "except:\n",
        "    path = ''\n",
        "\n",
        "print(f\"Path is {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "omhBufyrSTDt"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUYmQGxEi8XM",
        "outputId": "5f4c9635-3e46-40b6-d058-99b456b0ae49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW2U2MJAY_MW"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnqG78znbHWJ",
        "outputId": "953e4dbe-307c-43e2-a8e0-90f451379770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(404290,)\n",
            "(404290, 5)\n",
            "149263\n",
            "Ratio:\n",
            " 1: 36.9197853026293% \n",
            " 0: 63.08021469737071%\n"
          ]
        }
      ],
      "source": [
        "path = ''\n",
        "df = pd.read_csv(path+'train.csv')\n",
        "y = np.array(df['is_duplicate'])\n",
        "X = df[['id', 'qid1', 'qid2', 'question1', 'question2']]\n",
        "X = np.array(X)\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "ones = np.count_nonzero(y == 1)\n",
        "print(ones)\n",
        "print(f\"Ratio:\\n 1: {(ones/len(y)) * 100}% \\n 0: {(1-ones/len(y)) * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYYTtlNqbJka",
        "outputId": "08d2a683-40ca-44c2-9d80-91c0bb25605c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['What is the step by step guide to invest in share market in india?',\n",
              "       'What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n",
              "       'How can I increase the speed of my internet connection while using a VPN?',\n",
              "       ..., 'What is one coin?',\n",
              "       'What is the approx annual cost of living while studying in UIC Chicago, for an Indian student?',\n",
              "       'What is like to have sex with cousin?'], dtype=object)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train = df[['question1','question2' ]]\n",
        "X_train = np.array(X_train)\n",
        "X_train[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "-pVsecKfZB-1"
      },
      "outputs": [],
      "source": [
        "def train(train_dataset, test_dataset,feature_size, model, params={}, print_step=10):\n",
        "    train_len = len(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset,batch_size=64)\n",
        "    \n",
        "    if test_dataset != None:\n",
        "        test_len = len(test_dataset)\n",
        "        test_dataloader = DataLoader(test_dataset,batch_size=64)\n",
        "\n",
        "    \n",
        "\n",
        "    loss_fn = params['loss']\n",
        "    epochs = params['epochs']\n",
        "    optimizer = params['optimizer']\n",
        "\n",
        "    \n",
        "\n",
        "    model.to(device)\n",
        "    epoch_history = []\n",
        "    fold_train_loss_history, fold_val_loss_history = [], []\n",
        "    fold_val_acc_history = []\n",
        "    \n",
        "    for e in range(1,epochs+1):\n",
        "        \n",
        "        total_train_loss = 0\n",
        "        total_train_correct = 0\n",
        "        for batch_x, batch_y in train_dataloader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "    \n",
        "            # Compute prediction error\n",
        "            pred = torch.squeeze(model(batch_x))\n",
        "            loss = loss_fn(pred, batch_y)\n",
        "            \n",
        "            \n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #Save Training Loss and total correct\n",
        "            total_train_loss += loss.item()\n",
        "            out = (pred>0.5).float()\n",
        "            total_train_correct += sum(out==batch_y).float().sum()\n",
        "     \n",
        "        if e == 1 or e % print_step == 0:\n",
        "            with torch.no_grad():\n",
        "                epoch_history.append(e)\n",
        "\n",
        "                train_acc = total_train_correct / train_len\n",
        "                avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "                fold_train_loss_history.append(avg_train_loss)\n",
        "                if test_dataset != None:\n",
        "                    total_val_correct = 0\n",
        "                    total_val_loss = 0\n",
        "                    for val_x, val_y in test_dataloader:\n",
        "                        val_x, val_y = val_x.to(device), val_y.to(device)\n",
        "                        val_pred = torch.squeeze(model(val_x))\n",
        "                        val_out = (val_pred>0.5).float()\n",
        "                        val_loss = loss_fn(val_pred, val_y)\n",
        "                        total_val_loss += val_loss.cpu()\n",
        "                        total_val_correct += sum(val_out==val_y).float().sum()\n",
        "\n",
        "                    val_acc = total_val_correct / test_len\n",
        "                    avg_val_loss = total_val_loss / len(test_dataloader)\n",
        "                    fold_val_acc_history.append(val_acc.cpu())\n",
        "                    fold_val_loss_history.append(avg_val_loss)\n",
        "               \n",
        "\n",
        "                    print(f\"epoch: {e}, Train loss: {avg_train_loss:>4f},  Train Acc: {train_acc:>4f}, Val Loss: {avg_val_loss::>4f}, Val Acc: {val_acc:>4f}\")\n",
        "                else:\n",
        "                    print(f\"epoch: {e}, Train loss: {avg_train_loss:>4f},  Train Acc: {train_acc:>4f}\")   \n",
        "    return fold_train_loss_history, fold_val_loss_history, fold_val_acc_history, epoch_history,model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdugqPIi9iGm"
      },
      "source": [
        "# Glove50 Logsitic Regregression Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "XiOtdSh1UrYi"
      },
      "outputs": [],
      "source": [
        "class Glove50Dataset():\n",
        "    def __init__(self,X,y )-> None:\n",
        "        \n",
        "\n",
        "        word_embeddings = pd.read_csv(path+'glove.6B.50d.txt.zip',\n",
        "                               header=None, sep=' ', index_col=0,\n",
        "                               nrows=100000, compression='zip', encoding='utf-8', quoting=3)\n",
        "        # Build a dict that will map from string word to 50-dim vector\n",
        "        word_list = word_embeddings.index.values.tolist()\n",
        "        word2vec = OrderedDict(zip(word_list, word_embeddings.values))\n",
        "        \n",
        "\n",
        "        q1 = []\n",
        "        q2 = []\n",
        "        a = []\n",
        "        for (x1, x2),y1 in zip(X,y):\n",
        "          if isinstance(x1, str) and isinstance(x2, str):\n",
        "            q1.append(x1)\n",
        "            q2.append(x2)\n",
        "            a.append(y1)\n",
        "          else:\n",
        "            print(x1,x2)\n",
        "\n",
        "        pair_len = len(q1)\n",
        "\n",
        "        x_embed_q1 = np.zeros((pair_len, 50))\n",
        "        x_embed_q2 = np.zeros((pair_len, 50))        \n",
        "        for idx, (x1, x2) in enumerate(zip(q1,q2)):\n",
        "\n",
        "          question1 = x1.strip().split(\" \")\n",
        "          question2 = x2.strip().split(\" \")\n",
        "\n",
        "          for word in question1:\n",
        "            if word in word2vec:\n",
        "              x_embed_q1[idx] += word2vec[word]\n",
        "            x_embed_q1[idx] /= len(question1)\n",
        "\n",
        "          for word in question2:\n",
        "            if word in word2vec:\n",
        "              x_embed_q2[idx] += word2vec[word]\n",
        "            x_embed_q2[idx] /= len(question2)\n",
        "        \n",
        "\n",
        "        x_embed = np.concatenate((x_embed_q1,x_embed_q2),axis=1)\n",
        "        print(x_embed.shape)\n",
        "\n",
        "        self.feature_size = x_embed.shape[1]\n",
        "        self.x_train = torch.Tensor(x_embed)[:100000]\n",
        "        self.y_train = torch.Tensor(a)[:100000]\n",
        "        print(self.feature_size, self.x_train.shape, self.y_train.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_train.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_train[idx], self.y_train[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "5VqDBOcWSNev"
      },
      "outputs": [],
      "source": [
        "class LogisticModel(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(LogisticModel,self).__init__()\n",
        "        self.w = nn.Linear(input_size,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.sigmoid(self.w(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPiSkOTNY0rh",
        "outputId": "c9d8e0fa-59bd-4747-a5bb-2f44d7a7baf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How can I develop android app? nan\n",
            "How can I create an Android app? nan\n",
            "nan My Chinese name is Haichao Yu. What English name is most suitable for me considering the pronounciation of my Chinese name?\n",
            "(404287, 100)\n",
            "100 torch.Size([100000, 100]) torch.Size([100000])\n"
          ]
        }
      ],
      "source": [
        "dataset = Glove50Dataset(X_train,y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ruINwqrgU00"
      },
      "outputs": [],
      "source": [
        "#For Testing\n",
        "model = LogisticModel(dataset.feature_size) \n",
        "\n",
        "params = {  \"loss\": nn.BCELoss(),\n",
        "            \"optimizer\":torch.optim.Adam(model.parameters(), lr=1e-2,betas=(0.9, 0.999)),\n",
        "            \"epochs\": 60 }\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "kfold.split(dataset)\n",
        "train_loss_history, val_loss_history = [], []\n",
        "val_acc_history = []\n",
        "for fold, (train_idxs, test_idxs) in enumerate(kfold.split(dataset)):\n",
        "\n",
        "    print(f'Starting fold {fold}')\n",
        "    train_dataset = torch.utils.data.Subset(dataset,train_idxs)\n",
        "    test_dataset = torch.utils.data.Subset(dataset,test_idxs)\n",
        "\n",
        "\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for idx in range(len(test_dataset)):\n",
        "        input_data, target = test_dataset[idx]\n",
        "        x_test.append(input_data)\n",
        "        y_test.append(target)\n",
        "    x_test = torch.stack(x_test)\n",
        "    y_test = torch.tensor(y_test)\n",
        "\n",
        "\n",
        "    fold_train_loss_history, fold_val_loss_history, \\\n",
        "    fold_val_acc_history, epoch_history, _ = train(\\\n",
        "                            train_dataset=train_dataset,\n",
        "                            test_dataset=test_dataset,\n",
        "                            feature_size=dataset.feature_size,\n",
        "                            model=model,\n",
        "                            params = params)\n",
        "    \n",
        "\n",
        "    pred = torch.squeeze(model(x_test))\n",
        "    y_pred = (pred>0.5).float()\n",
        "    print(f\"F1 score: {f1_score(y_test, y_pred, pos_label=1, average='binary')}\")\n",
        "    print(f\"AUC score: {roc_auc_score(y_test.detach().numpy(), model.forward(x_test).detach().numpy())}\")\n",
        "\n",
        "    \n",
        "    train_loss_history.append(fold_train_loss_history)\n",
        "    val_loss_history.append(fold_val_loss_history)\n",
        "    val_acc_history.append(fold_val_acc_history) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITY2hjQKTDM4"
      },
      "source": [
        "# Sentence Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Evn_hzLWDRoh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: sentencepiece in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.6.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (2.0.1)\n",
            "Requirement already satisfied: torchvision in ./venv/lib/python3.10/site-packages (from sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./venv/lib/python3.10/site-packages (from sentence-transformers) (4.29.2)\n",
            "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (8.5.0.96)\n",
            "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.14.3)\n",
            "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.2.10.91)\n",
            "Requirement already satisfied: triton==2.0.0 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.91)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.0.1)\n",
            "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (59.6.0)\n",
            "Requirement already satisfied: wheel in ./venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.40.0)\n",
            "Requirement already satisfied: lit in ./venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.5)\n",
            "Requirement already satisfied: cmake in ./venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.26.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.5.5)\n",
            "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_EjEhMElDiDf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaytea/Code_Workspace/Labs/Coen346/NLP-quora-duplicate-detection/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9rbCmavDOAp",
        "outputId": "2eac146f-cb76-440d-8d7d-a8c20371d93c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "class SentenceTransformerDataset():\n",
        "    def __init__(self,X,y, pair_len=0)-> None:\n",
        "        \n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        \n",
        "        q1 = []\n",
        "        q2 = []\n",
        "        a = []\n",
        "        for (x1, x2),y1 in zip(X,y):\n",
        "          if isinstance(x1, str) and isinstance(x2, str):\n",
        "            q1.append(x1)\n",
        "            q2.append(x2)\n",
        "            a.append(y1)\n",
        "          else:\n",
        "            print(x1,x2)\n",
        "\n",
        "        pair_len = pair_len if pair_len else len(q1)\n",
        "\n",
        "        q1, q2, a = q1[:pair_len], q2[:pair_len], a[:pair_len]\n",
        "        x_embed_q1 = np.zeros((pair_len, 384))[:pair_len]\n",
        "        x_embed_q2 = np.zeros((pair_len, 384)) [:pair_len]\n",
        "\n",
        "        pool = model.start_multi_process_pool()\n",
        "\n",
        "\n",
        "        #Start the multi-process pool on all available CUDA devices\n",
        "        pool = model.start_multi_process_pool()\n",
        "\n",
        "        step_size = 10000\n",
        "        for start in tqdm(range(0, pair_len, step_size),total=pair_len//step_size):\n",
        "          stop = start + step_size if (start + step_size) < pair_len else (pair_len - 1)\n",
        "          x_embed_q1[start:stop] = model.encode_multi_process(q1[start:stop], pool)\n",
        "          x_embed_q2[start:stop] = model.encode_multi_process(q2[start:stop], pool)\n",
        "\n",
        "        #Optional: Stop the proccesses in the pool\n",
        "        model.stop_multi_process_pool(pool)\n",
        "\n",
        "        x_embed = np.concatenate((x_embed_q1,x_embed_q2),axis=1)\n",
        "        \n",
        "        print(x_embed.shape)\n",
        "\n",
        "        self.feature_size = x_embed.shape[1]\n",
        "        self.setences = [q1,q2]\n",
        "        self.x_train = torch.Tensor(x_embed)\n",
        "        self.y_train = torch.Tensor(a)\n",
        "        print(self.feature_size, self.x_train.shape, self.y_train.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x_train.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_train[idx], self.y_train[idx]\n",
        "\n",
        "\n",
        "\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Kqx5xWXThD",
        "outputId": "4c8697e0-8165-4e7b-b37c-921f92824178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Dataset from Drive\n",
            "Dataset Load!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "create_bert_encoding = False\n",
        "file_name = 'bert_encoding_max.pkl'\n",
        "\n",
        "\n",
        "run = 'y'\n",
        "if create_bert_encoding: \n",
        "  if os.path.isfile(path+file_name):\n",
        "    run = input(f\"'{file_name}' exists in the drive! Would you like to replace? (y/n)\")\n",
        "  if run == 'y':\n",
        "    bert_dataset = SentenceTransformerDataset(X_train,y,pair_len=0)\n",
        "    # Save the object to a file\n",
        "    print(f'Writing Dataset to Drive')\n",
        "    with open(path+file_name, 'wb') as f:\n",
        "        pickle.dump(bert_dataset, f)\n",
        "    print(f'Dataset Written!')\n",
        "\n",
        "if not create_bert_encoding or run != 'y':\n",
        "  # Load the object from the file\n",
        "  if not os.path.isfile(path+file_name):\n",
        "    raise ValueError(\"No Dataset in drive\")\n",
        "  else:\n",
        "    print(f'Loading Dataset from Drive')\n",
        "    with open(path+file_name, 'rb') as f:\n",
        "        bert_dataset = pickle.load(f)\n",
        "    print(f'Dataset Load!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6unhCuBzzih3",
        "outputId": "84e0b1ff-ebce-4587-abf2-63a2a0226e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train|Val Percent: 0.8|0.19999999999999996\n",
            "Train|Val Size: 323429|80858\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "train_percent = .8\n",
        "val_percent = 1 - train_percent\n",
        "sample_size = len(bert_dataset)\n",
        "train_size = int(sample_size * train_percent)\n",
        "val_size = sample_size - train_size\n",
        "\n",
        "\n",
        "print(f'Train|Val Percent: {train_percent}|{val_percent}')\n",
        "print(f'Train|Val Size: {train_size}|{val_size}')\n",
        "# Split the data into train and test sets\n",
        "train_dataset, test_dataset = random_split(bert_dataset, [train_size, val_size])\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "for idx in range(len(test_dataset)):\n",
        "    input_data, target = test_dataset[idx]\n",
        "    x_test.append(input_data)\n",
        "    y_test.append(target)\n",
        "x_test = torch.stack(x_test)\n",
        "y_test = torch.tensor(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of 0s: 203972\n",
            "Number of 1s: 119457\n",
            "Training data Ratio:\n",
            " 1: 36.93453586413092% \n",
            " 0: 63.06546413586908%\n"
          ]
        }
      ],
      "source": [
        "# Check data balance\n",
        "indices = train_dataset.indices\n",
        "targets = train_dataset.dataset.y_train\n",
        "\n",
        "num_zeros = 0\n",
        "num_ones = 0\n",
        "\n",
        "for index in indices:\n",
        "    label = targets[index]\n",
        "    if label == 0:\n",
        "        num_zeros += 1\n",
        "    elif label == 1:\n",
        "        num_ones += 1\n",
        "\n",
        "print(\"Number of 0s:\", num_zeros)\n",
        "print(\"Number of 1s:\", num_ones)\n",
        "ratio = num_ones / (num_ones + num_zeros)\n",
        "print(f\"Training data Ratio:\\n 1: {ratio * 100}% \\n 0: {(1-ratio) * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a3dda4d00b524a16b8ddfabea43f3c07",
            "a119a8e7f5574e1e85965abd5749b91b",
            "2d79d95f737b4c34905e0a7f40f91220",
            "69d4cb37419f4a448140d7d1498a2fd5",
            "e194987ed32a4bd6b0b6d18ef0cf1da3",
            "64b3ef56796747fb8d9ef8cf8b4e0171",
            "5021a6b89ed945bf94483e1f9473db3f",
            "f1c5786153924fce9a3a52aac1c1570d",
            "4b2c4fc257474af282c8ba74bc140757",
            "3d79872fbecd4fc6a59cf9b3a01d3581",
            "a99922cd7d32404fb1c746b6fe575056"
          ]
        },
        "id": "5UW1ea5nkIe3",
        "outputId": "b6d29dd5-62a5-4323-dc0f-5a1aca302035"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 80858/80858 [00:03<00:00, 23588.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Sim Accuracy: 0.7818274011229563\n",
            "F1 score: 0.7218692354990777\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import util\n",
        "import math\n",
        "\n",
        "feature_size = bert_dataset.feature_size//2\n",
        "total = 0\n",
        "cos_scores = []\n",
        "param = 0.8\n",
        "for x, y in tqdm(test_dataset, total=len(test_dataset)):\n",
        "  \n",
        "  q1 = x[:feature_size]\n",
        "  q2 = x[feature_size:]\n",
        "  cosine = util.cos_sim(q1, q2).item()\n",
        "  cos_scores.append(1 if cosine >=param else 0)\n",
        "  if cosine >= param and y == 1 or cosine < param and y == 0:\n",
        "    total += 1\n",
        "\n",
        "percent = total / len(test_dataset)\n",
        "print(f\"Cosine Sim Accuracy: {percent}\")\n",
        "print(f\"F1 score: {f1_score(y_test, cos_scores, average='binary')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJBS83PJeFmG",
        "outputId": "5c673554-6cdb-4b40-fb43-7973673eb955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, Train loss: 0.572727,  Train Acc: 0.708653, Val Loss: 0.567628, Val Acc: 0.713832\n",
            "epoch: 2, Train loss: 0.566584,  Train Acc: 0.713000, Val Loss: 0.566740, Val Acc: 0.714215\n",
            "epoch: 4, Train loss: 0.565799,  Train Acc: 0.713464, Val Loss: 0.566353, Val Acc: 0.714561\n",
            "epoch: 6, Train loss: 0.565677,  Train Acc: 0.713585, Val Loss: 0.566254, Val Acc: 0.714957\n",
            "epoch: 8, Train loss: 0.565646,  Train Acc: 0.713588, Val Loss: 0.566215, Val Acc: 0.715217\n",
            "epoch: 10, Train loss: 0.565636,  Train Acc: 0.713498, Val Loss: 0.566196, Val Acc: 0.715093\n",
            "F1 score: 0.5459705551942292\n",
            "AUC score: 0.7478245615112822\n"
          ]
        }
      ],
      "source": [
        "#For Training Simple Logisitc Regression with Bert\n",
        "\n",
        "model = LogisticModel(bert_dataset.feature_size) \n",
        "\n",
        "params = {  \"loss\": nn.BCELoss(),\n",
        "            \"optimizer\":torch.optim.Adam(model.parameters(), lr=1e-2,betas=(0.9, 0.999)),\n",
        "            \"epochs\": 10 }\n",
        "\n",
        "\n",
        "fold_train_loss_history, fold_val_loss_history, \\\n",
        "    fold_val_acc_history, epoch_history, _ = train(\\\n",
        "                            train_dataset=train_dataset,\n",
        "                            test_dataset=test_dataset,\n",
        "                            feature_size=bert_dataset.feature_size,\n",
        "                            model=model,\n",
        "                            params = params,\n",
        "                            print_step=2)\n",
        "\n",
        "pred = torch.squeeze(model(x_test))\n",
        "y_pred = (pred>0.5).float()\n",
        "print(f\"F1 score: {f1_score(y_test, y_pred, pos_label=1, average='binary')}\")\n",
        "print(f\"AUC score: {roc_auc_score(y_test.detach().numpy(), model.forward(x_test).detach().numpy())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48\n"
          ]
        }
      ],
      "source": [
        "print(bert_dataset.feature_size//2//2//2//2)\n",
        "class LargerBertNetwork(nn.Module):\n",
        "    def __init__(self,input_size):\n",
        "        super(LargerBertNetwork,self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size,384)\n",
        "        self.layer_2 = nn.Linear(384,192)\n",
        "        self.layer_3 = nn.Linear(192,96)\n",
        "        self.layer_4 = nn.Linear(96,48)\n",
        "        self.layer_f = nn.Linear(48,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        x = F.relu(self.layer_3(x))\n",
        "        x = F.relu(self.layer_4(x))\n",
        "        return F.sigmoid(self.layer_f(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, Train loss: 0.621545,  Train Acc: 0.649330, Val Loss: 0.579493, Val Acc: 0.713485\n",
            "epoch: 2, Train loss: 0.553394,  Train Acc: 0.727424, Val Loss: 0.532930, Val Acc: 0.740335\n",
            "epoch: 3, Train loss: 0.498440,  Train Acc: 0.761104, Val Loss: 0.465398, Val Acc: 0.777301\n",
            "epoch: 4, Train loss: 0.423719,  Train Acc: 0.799471, Val Loss: 0.397122, Val Acc: 0.811682\n",
            "epoch: 5, Train loss: 0.370333,  Train Acc: 0.828006, Val Loss: 0.365030, Val Acc: 0.827648\n",
            "epoch: 6, Train loss: 0.343200,  Train Acc: 0.842939, Val Loss: 0.350163, Val Acc: 0.835848\n",
            "epoch: 7, Train loss: 0.326063,  Train Acc: 0.852054, Val Loss: 0.341370, Val Acc: 0.841042\n",
            "epoch: 8, Train loss: 0.312984,  Train Acc: 0.859366, Val Loss: 0.335446, Val Acc: 0.844715\n",
            "epoch: 9, Train loss: 0.302110,  Train Acc: 0.865430, Val Loss: 0.331268, Val Acc: 0.847275\n",
            "epoch: 10, Train loss: 0.292616,  Train Acc: 0.870488, Val Loss: 0.328285, Val Acc: 0.849440\n",
            "epoch: 11, Train loss: 0.284088,  Train Acc: 0.875323, Val Loss: 0.326222, Val Acc: 0.851159\n",
            "epoch: 12, Train loss: 0.276280,  Train Acc: 0.879442, Val Loss: 0.324830, Val Acc: 0.852754\n",
            "epoch: 13, Train loss: 0.269027,  Train Acc: 0.883455, Val Loss: 0.324056, Val Acc: 0.853558\n",
            "epoch: 14, Train loss: 0.262200,  Train Acc: 0.886992, Val Loss: 0.323808, Val Acc: 0.854659\n",
            "epoch: 15, Train loss: 0.255739,  Train Acc: 0.890427, Val Loss: 0.323976, Val Acc: 0.855834\n",
            "epoch: 16, Train loss: 0.249589,  Train Acc: 0.893615, Val Loss: 0.324550, Val Acc: 0.856242\n",
            "epoch: 17, Train loss: 0.243689,  Train Acc: 0.896611, Val Loss: 0.325492, Val Acc: 0.856885\n",
            "epoch: 18, Train loss: 0.237987,  Train Acc: 0.899728, Val Loss: 0.326828, Val Acc: 0.857318\n",
            "epoch: 19, Train loss: 0.232487,  Train Acc: 0.902452, Val Loss: 0.328489, Val Acc: 0.857479\n",
            "epoch: 20, Train loss: 0.227136,  Train Acc: 0.905160, Val Loss: 0.330451, Val Acc: 0.857565\n",
            "F1 score: 0.8094883628604039\n",
            "AUC score: 0.9335005512603727\n"
          ]
        }
      ],
      "source": [
        "#For Training Largers Model with Bert\n",
        "model = LargerBertNetwork(bert_dataset.feature_size) \n",
        "\n",
        "params = {  \"loss\": nn.BCELoss(),\n",
        "            \"optimizer\":torch.optim.Adam(model.parameters(), lr=1e-5,betas=(0.9, 0.999)),\n",
        "            \"epochs\": 20 }\n",
        "\n",
        "\n",
        "fold_train_loss_history, fold_val_loss_history, \\\n",
        "    fold_val_acc_history, epoch_history, _ = train(\\\n",
        "                            train_dataset=train_dataset,\n",
        "                            test_dataset=test_dataset,\n",
        "                            feature_size=bert_dataset.feature_size,\n",
        "                            model=model,\n",
        "                            params = params,\n",
        "                            print_step=1)\n",
        "\n",
        "pred = torch.squeeze(model(x_test))\n",
        "y_pred = (pred>0.5).float()\n",
        "print(f\"F1 score: {f1_score(y_test, y_pred, pos_label=1, average='binary')}\")\n",
        "print(f\"AUC score: {roc_auc_score(y_test.detach().numpy(), model.forward(x_test).detach().numpy())}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PdugqPIi9iGm"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d79d95f737b4c34905e0a7f40f91220": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c5786153924fce9a3a52aac1c1570d",
            "max": 80858,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b2c4fc257474af282c8ba74bc140757",
            "value": 80858
          }
        },
        "3d79872fbecd4fc6a59cf9b3a01d3581": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2c4fc257474af282c8ba74bc140757": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5021a6b89ed945bf94483e1f9473db3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64b3ef56796747fb8d9ef8cf8b4e0171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d4cb37419f4a448140d7d1498a2fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d79872fbecd4fc6a59cf9b3a01d3581",
            "placeholder": "​",
            "style": "IPY_MODEL_a99922cd7d32404fb1c746b6fe575056",
            "value": " 80858/80858 [00:10&lt;00:00, 6166.60it/s]"
          }
        },
        "a119a8e7f5574e1e85965abd5749b91b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64b3ef56796747fb8d9ef8cf8b4e0171",
            "placeholder": "​",
            "style": "IPY_MODEL_5021a6b89ed945bf94483e1f9473db3f",
            "value": "100%"
          }
        },
        "a3dda4d00b524a16b8ddfabea43f3c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a119a8e7f5574e1e85965abd5749b91b",
              "IPY_MODEL_2d79d95f737b4c34905e0a7f40f91220",
              "IPY_MODEL_69d4cb37419f4a448140d7d1498a2fd5"
            ],
            "layout": "IPY_MODEL_e194987ed32a4bd6b0b6d18ef0cf1da3"
          }
        },
        "a99922cd7d32404fb1c746b6fe575056": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e194987ed32a4bd6b0b6d18ef0cf1da3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c5786153924fce9a3a52aac1c1570d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
